{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# mounting the drive\n",
    "from google.colab import drive\n",
    "drive.mount('/content/drive/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing libraries and modules\n",
    "import tensorflow as tf\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reading the data in a dataframe df\n",
    "df = pd.read_csv('/content/Sentiment Analysis Dataset (1).csv', encoding='latin-1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# printing the dataframe \n",
    "df.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# drop the unnecessary columns\n",
    "df.drop('ItemID', axis=1, inplace=True)\n",
    "df.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# veryfying the sentiment values\n",
    "# 1 is positive sentiment and 0 is negative sentiment\n",
    "df['Sentiment'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pre-processing the data\n",
    "# define a function to remove the @mentions and other useless text from the tweets\n",
    "\n",
    "import re\n",
    "\n",
    "def text_cleaning(text):\n",
    "  text = re.sub(r'@[A-Za-z0-9]+', '', text)     # removing @mentions\n",
    "  text = re.sub(r'@[A-Za-zA-Z0-9]+', '', text)  # removing @mentions \n",
    "  text = re.sub(r'@[A-Za-z]+', '', text)        # removing @mentions\n",
    "  text = re.sub(r'@[-)]+', '', text)            # removing @mentions\n",
    "  text = re.sub(r'#', '', text )                # removing '#' sign\n",
    "  text = re.sub(r'RT[\\s]+', '', text)           # removing RT\n",
    "  text = re.sub(r'https?\\/\\/\\S+', '', text)     # removing the hyper link\n",
    "  text = re.sub(r'&[a-z;]+', '', text)          # removing '&gt;'\n",
    "\n",
    "  return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# applying the text cleaning function on tweets\n",
    "df['SentimentText'] = df['SentimentText'].apply(text_cleaning)\n",
    "df.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# splitting the data into training and testing data\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "x_train, x_test, y_train, y_test = train_test_split(df['SentimentText'].values, df['Sentiment'].values, test_size=0.30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# checking the data split\n",
    "print('sentiment Text: ', x_train[0])\n",
    "print('sentiment: ', y_train[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# converting the strings into integers using Tokenizer \n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# instantiating the tokenizer\n",
    "max_vocab = 20000000\n",
    "tokenizer = Tokenizer(num_words=max_vocab)\n",
    "tokenizer.fit_on_texts(x_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# checking the word index and find out the vocabulary of the dataset\n",
    "wordidx = tokenizer.word_index\n",
    "V = len(wordidx)\n",
    "print('The size of datatset vocab is: ', V)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# converting train and test sentences into sequences\n",
    "train_seq = tokenizer.texts_to_sequences(x_train)\n",
    "test_seq = tokenizer.texts_to_sequences(x_test)\n",
    "print('Training sequence: ', train_seq[0])\n",
    "print('Testing sequence: ', test_seq[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# padding the sequences to get equal length sequence because its conventional to use same size sequences\n",
    "# padding the traing sequence\n",
    "pad_train = pad_sequences(train_seq)\n",
    "T = pad_train.shape[1]\n",
    "print('The length of training sequence is: ', T)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# padding the test sequence\n",
    "pad_test = pad_sequences(test_seq, maxlen=T)\n",
    "print('The length of testing sequence is: ', pad_test.shape[1])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# building the model\n",
    "\n",
    "from tensorflow.keras.layers import Input, Dense, Embedding, LSTM, GlobalMaxPooling1D\n",
    "from tensorflow.keras.models import Model\n",
    "\n",
    "D = 20 \n",
    "M = 15\n",
    "\n",
    "i = Input (shape=(T, ))   \n",
    "x = Embedding(V+1, D)(i)    # V+1 because the indexing of the words in vocab (V) start from 1 not 0\n",
    "x = LSTM(M, return_sequences=True)(x)\n",
    "x = GlobalMaxPooling1D()(x)\n",
    "x = Dense(32, activation='relu')(x)\n",
    "x = Dense(1, activation='sigmoid')(x)\n",
    "\n",
    "model = Model(i,x)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compiling the model\n",
    "model.compile(optimizer='adam', \n",
    "              loss='binary_crossentropy', \n",
    "              metrics=['accuracy'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# training the model\n",
    "r = model.fit(pad_train , y_train , validation_data = (pad_test , y_test) , epochs = 1)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predicting the sentiment of any text\n",
    "\n",
    "def predict_sentiment(text):\n",
    "  # preprocessing the given text \n",
    "  text_seq = tokenizer.texts_to_sequences(text)\n",
    "  text_pad = pad_sequences(text_seq, maxlen=T)\n",
    "\n",
    "  # predicting the class\n",
    "  predicted_sentiment = model.predict(text_pad).round()\n",
    "\n",
    "  if predicted_sentiment == 1.0:\n",
    "    return(print('It is a positive sentiment'))\n",
    "  else:\n",
    "    return(print('It is a negative sentiment'))\n",
    "\n",
    "text = ['I am not sad today']\n",
    "predict_sentiment(text)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "plaintext"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
